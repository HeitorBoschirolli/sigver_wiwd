{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WD Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduzindo os resultados de [3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Montando o Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo [1], cada autor possui 24 assinaturas genuínas e 30 forjas, mas aqui foi verificado que para alguns autores estão faltando algumas forjas (pelo menos nos features disponibilizados no github).\n",
    "\n",
    "Como a quantidade de assinaturas faltando é pequena, a importância desse fato foi desconsiderada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing forged samples: 95 out of 8905\n",
      "missing genuine samples: 0 out of 7200\n"
     ]
    }
   ],
   "source": [
    "exploitation_real = list()\n",
    "exploitation_forg = list()\n",
    "\n",
    "sig_files = os.listdir('./features/gpds_signet/exploitation/')\n",
    "sig_files.sort()\n",
    "\n",
    "total_forg = 0\n",
    "total_real = 0\n",
    "incomplete_forg = 0\n",
    "incomplete_real = 0\n",
    "\n",
    "for i in range(300):\n",
    "    sigs = loadmat('./features/gpds_signet/exploitation/' + sig_files[i])['features']\n",
    "    exploitation_forg.append(sigs)\n",
    "    incomplete_forg += 30 - sigs.shape[0]\n",
    "    total_forg += sigs.shape[0]\n",
    "\n",
    "for i in range(300, 600):\n",
    "    sigs = loadmat('./features/gpds_signet/exploitation/' + sig_files[i])['features']\n",
    "    exploitation_real.append(sigs)\n",
    "    incomplete_real += 24 - sigs.shape[0]\n",
    "    total_real += sigs.shape[0]\n",
    "        \n",
    "print 'missing forged samples: ' + str(incomplete_forg) + ' out of ' + str(total_forg)\n",
    "print 'missing genuine samples: ' + str(incomplete_real) + ' out of ' + str(total_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing forged samples: 0 out of 1500\n",
      "missing genuine samples: 0 out of 1200\n"
     ]
    }
   ],
   "source": [
    "validation_real = list()\n",
    "validation_forg = list()\n",
    "\n",
    "sig_files = os.listdir('./features/gpds_signet/validation/')\n",
    "\n",
    "sig_files.sort()\n",
    "\n",
    "total_forg = 0\n",
    "total_real = 0\n",
    "incomplete_forg = 0\n",
    "incomplete_real = 0\n",
    "\n",
    "for i in range(50):\n",
    "    sigs = loadmat('./features/gpds_signet/validation/' + sig_files[i])['features']\n",
    "    validation_forg.append(sigs)\n",
    "    incomplete_forg += 30 - sigs.shape[0]\n",
    "    total_forg += sigs.shape[0]\n",
    "\n",
    "for i in range(50, 100):\n",
    "    sigs = loadmat('./features/gpds_signet/validation/' + sig_files[i])['features']\n",
    "    validation_real.append(sigs)\n",
    "    incomplete_real += 24 - sigs.shape[0]\n",
    "    total_real += sigs.shape[0]\n",
    "        \n",
    "print 'missing forged samples: ' + str(incomplete_forg) + ' out of ' + str(total_forg)\n",
    "print 'missing genuine samples: ' + str(incomplete_real) + ' out of ' + str(total_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing forged samples: 21 out of 15909\n",
      "missing genuine samples: 0 out of 12744\n"
     ]
    }
   ],
   "source": [
    "development_real = list()\n",
    "development_forg = list()\n",
    "\n",
    "sig_files = os.listdir('./features/gpds_signet/development/')\n",
    "sig_files.sort()\n",
    "\n",
    "total_forg = 0\n",
    "total_real = 0\n",
    "incomplete_forg = 0\n",
    "incomplete_real = 0\n",
    "\n",
    "for i in range(531):\n",
    "    sigs = loadmat('./features/gpds_signet/development/' + sig_files[i])['features']\n",
    "    development_forg.append(sigs)\n",
    "    incomplete_forg += 30 - sigs.shape[0]\n",
    "    total_forg += sigs.shape[0]\n",
    "\n",
    "for i in range(531, 531 * 2):\n",
    "    sigs = loadmat('./features/gpds_signet/development/' + sig_files[i])['features']\n",
    "    development_real.append(sigs)\n",
    "    incomplete_real += 24 - sigs.shape[0]\n",
    "    total_real += sigs.shape[0]\n",
    "        \n",
    "print 'missing forged samples: ' + str(incomplete_forg) + ' out of ' + str(total_forg)\n",
    "print 'missing genuine samples: ' + str(incomplete_real) + ' out of ' + str(total_real)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como sanity check é bom verificar se o Exploitation, Development e Validation possuem 300, 531 e 50 autores respectivavmente.\n",
    "\n",
    "Além disso, cada elemento das listas deve conter o formato (x, 2048). Somente o primeiro será verificado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(exploitation_forg) == 300\n",
    "assert len(exploitation_real) == 300\n",
    "assert len(validation_real) == 50\n",
    "assert len(validation_forg) == 50\n",
    "assert len(development_forg) == 531\n",
    "assert len(development_real) == 531\n",
    "\n",
    "assert exploitation_forg[0].shape == (exploitation_forg[0].shape[0], 2048)\n",
    "assert exploitation_real[0].shape == (exploitation_real[0].shape[0], 2048)\n",
    "assert validation_real[0].shape == (validation_real[0].shape[0], 2048)\n",
    "assert validation_forg[0].shape == (validation_forg[0].shape[0], 2048)\n",
    "assert development_forg[0].shape == (development_forg[0].shape[0], 2048)\n",
    "assert development_real[0].shape == (development_real[0].shape[0], 2048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encontrando os hiperparâmetros do SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em [1] são usados 10 autores de D seguindo o mesmo protocolo do Exploitation Set para procurar pelos hiperparâmetros do SVM. Em [2] não é mencionado como, nem se, os hiperparâmetros são buscados, acredito que eles sejam buscados no Validation set ou que os mesmos valores encontrados em [1] sejam usados.\n",
    "\n",
    "Existe um problema com a abordagem de [1]. Como as amostras negativas contém assinaturas genuínas de todos os autores de D, elas contém assinaturas genuínas do autor para o qual está se fazendo a validacão como amostras negativas (o que é incorreto). Decidi portanto usado o Validadtion set para buscas os hiperparâmetros do SVM mesmo sem saber o procedimento usado em [2].\n",
    "\n",
    "Tanto em [1] quanto em [2], A busca por hiperparâmetros é feita apenas com o kernel RBF.\n",
    "\n",
    "Em [3] não é feita uma busca hiperparâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7434, 2048)\n"
     ]
    }
   ],
   "source": [
    "# The negative samples are genuine signatures of authors pron the development set\n",
    "train_negatives = list()\n",
    "for i in range(len(development_forg)):\n",
    "    author = development_real[i]\n",
    "    train_negatives += list(author[:14])\n",
    "    \n",
    "train_negatives = np.array(train_negatives)\n",
    "print train_negatives.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author 1 out of 50\n",
      "author 2 out of 50\n",
      "author 3 out of 50\n",
      "author 4 out of 50\n",
      "author 5 out of 50\n",
      "author 6 out of 50\n",
      "author 7 out of 50\n",
      "author 8 out of 50\n",
      "author 9 out of 50\n",
      "author 10 out of 50\n",
      "author 11 out of 50\n",
      "author 12 out of 50\n",
      "author 13 out of 50\n",
      "author 14 out of 50\n",
      "author 15 out of 50\n",
      "author 16 out of 50\n",
      "author 17 out of 50\n",
      "author 18 out of 50\n",
      "author 19 out of 50\n",
      "author 20 out of 50\n",
      "author 21 out of 50\n",
      "author 22 out of 50\n",
      "author 23 out of 50\n",
      "author 24 out of 50\n",
      "author 25 out of 50\n",
      "author 26 out of 50\n",
      "author 27 out of 50\n",
      "author 28 out of 50\n",
      "author 29 out of 50\n",
      "author 30 out of 50\n",
      "author 31 out of 50\n",
      "author 32 out of 50\n",
      "author 33 out of 50\n",
      "author 34 out of 50\n",
      "author 35 out of 50\n",
      "author 36 out of 50\n",
      "author 37 out of 50\n",
      "author 38 out of 50\n",
      "author 39 out of 50\n",
      "author 40 out of 50\n",
      "author 41 out of 50\n",
      "author 42 out of 50\n",
      "author 43 out of 50\n",
      "author 44 out of 50\n",
      "author 45 out of 50\n",
      "author 46 out of 50\n",
      "author 47 out of 50\n",
      "author 48 out of 50\n",
      "author 49 out of 50\n",
      "author 50 out of 50\n",
      "[[ 45.29999542  45.29999542  45.29999542]\n",
      " [ 44.29999924  44.29999924  44.32500076]\n",
      " [ 42.60000229  43.15000534  43.20000076]]\n"
     ]
    }
   ],
   "source": [
    "score_counter = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "], dtype='float32')\n",
    "\n",
    "for author_id in range(50):\n",
    "    print 'author ' + str(author_id + 1) + ' out of 50'\n",
    "    for gamma_index, gamma in enumerate([-9, -11, -13]):\n",
    "        for c_index, C in enumerate([1, 3, 10]):\n",
    "            print '%.2f %% \\r' % ((c_index + gamma_index * 5)/19.0 * 100),\n",
    "            \n",
    "            x_train = np.concatenate((validation_real[author_id][:14], train_negatives))\n",
    "            y_train = np.concatenate((\n",
    "                np.zeros(validation_real[author_id][:14].shape[0]), \n",
    "                np.ones(train_negatives.shape[0])\n",
    "            ))\n",
    "\n",
    "            x_test = np.concatenate((validation_real[author_id][14:], validation_forg[author_id]))\n",
    "            y_test = np.concatenate((\n",
    "                np.zeros(validation_real[author_id][14:].shape[0]),\n",
    "                np.ones(validation_forg[author_id].shape[0])\n",
    "            ))\n",
    "\n",
    "            scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "            scaler.transform(x_train)\n",
    "            scaler.transform(x_test)\n",
    "\n",
    "            clf = svm.SVC(\n",
    "                kernel='rbf', \n",
    "                C=C,\n",
    "                gamma=2**(gamma),\n",
    "                # it is advisable to set probability=False and use decision_function instead of predict_proba\n",
    "                probability=False,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            clf.fit(x_train, y_train)\n",
    "            score_counter[gamma_index][c_index] += clf.score(x_test, y_test)\n",
    "\n",
    "print score_counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em [1] encontra-se C = 1 e gamma = 2⁻¹² para o kernel RBF. Para o kernel linear usa-se C=1.\n",
    "\n",
    "Em [2] encontra-se C = 1 egamma = 2⁻¹¹ para o kernel RBF. Para o kernel linear usa-se C=1.\n",
    "\n",
    "Em [3] usa-se os mesmos valores de [2].\n",
    "\n",
    "Aqui encontra-se C = 1 e gamma = 2⁻⁹. Para o kernel RBF. Não foi usado o kernel linear porque ele sempre tinha um desempenho inferior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinando e testando o modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treino e teste são feitos individualmente para cada autor do Exploitation set.\n",
    "\n",
    "\n",
    "##### Treino\n",
    "Como amostras positivas serão usadas 12 das 24 assinaturas genuínas disponíveis para cada autor no Exploitation set. Como amostras negativas serão usadas 12 assinaturas de CADA autor do development set (isso é feito porque não faz sentido usar forjas durante o treino).\n",
    "\n",
    "##### Teste\n",
    "Como amostras positivas serão usadas as 12 assinaturas genuínas restantes e como negativas serão usadas todas as forjas disponíveis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como todos os autores usam as mesmas amostras negativas, esse conjunto será montado antes de percorrer os autores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run 1\n",
      "0.032082688916         \n",
      "run 2\n",
      "0.0338927203065        \n",
      "run 3\n",
      "0.034681097638         \n",
      "run 4\n",
      "0.0302518853007        \n",
      "run 5\n",
      "0.0334306567209        \n",
      "run 6\n",
      "0.0355178869662        \n",
      "run 7\n",
      "0.0307833728638        \n",
      "run 8\n",
      "257 out of 300\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0ef54202af69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         )\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0my_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hboschirolli/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m         \u001b[0;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/hboschirolli/anaconda2/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36m_dense_fit\u001b[0;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[1;32m    254\u001b[0m                 \u001b[0mcache_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m                 max_iter=self.max_iter, random_seed=random_seed)\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_warn_from_fit_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "eers = list()\n",
    "\n",
    "for run in range(10):\n",
    "    print 'run', (run + 1)\n",
    "    sum_eer = 0\n",
    "    sum_eer_size = 0\n",
    "\n",
    "    for author_id in range(300):\n",
    "        author_exploitation_real = shuffle(exploitation_real[author_id])\n",
    "        author_exploitation_forg = shuffle(exploitation_forg[author_id])\n",
    "        print str(author_id + 1) + ' out of 300\\r',\n",
    "\n",
    "        x_train = np.concatenate((author_exploitation_real[:12], train_negatives))\n",
    "        y_train = np.concatenate((\n",
    "            np.zeros(author_exploitation_real[:12].shape[0]),\n",
    "            np.ones(train_negatives.shape[0])\n",
    "        ))\n",
    "\n",
    "        x_test = np.concatenate((author_exploitation_real[12:], author_exploitation_forg))\n",
    "        y_test = np.concatenate((\n",
    "            np.zeros(author_exploitation_real[12:].shape[0]),\n",
    "            np.ones(author_exploitation_forg.shape[0])\n",
    "        ))\n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "        scaler.transform(x_train)\n",
    "        scaler.transform(x_test)\n",
    "\n",
    "        clf = svm.SVC(\n",
    "            kernel='rbf', \n",
    "            C=1,\n",
    "            gamma=2**(-9), # -9 or -11\n",
    "            # it is advisable to set probability=False and use decision_function instead of predict_proba\n",
    "            probability=False,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        y_scores = clf.decision_function(x_test)\n",
    "        fpr, tpr, threshold = metrics.roc_curve(y_test, y_scores, pos_label=1)\n",
    "        fnr = 1 - tpr\n",
    "        eer_index = np.argmin(np.absolute(fnr - fpr))\n",
    "        eer = (fpr[eer_index] + fnr[eer_index]) / 2\n",
    "        sum_eer += eer\n",
    "        sum_eer_size += 1\n",
    "    print '                       \\r',\n",
    "    eers.append(sum_eer / sum_eer_size)\n",
    "    print sum_eer / sum_eer_size\n",
    "\n",
    "eers = np.array(eers)\n",
    "print 'final results:'\n",
    "print 'mean eer:', eers.mean()\n",
    "print 'stddev:', eers.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Kernel linear\n",
    "<strong>[3]</strong>: EER=3.91+-0.64 (média, desvio padrão). <br> \n",
    "<strong>Aqui</strong>: Não testado.\n",
    "\n",
    "##### Kernel RBF\n",
    "<strong>[3]</strong>: EER=3.13+-0.46 (média, desvio padrão). <br>\n",
    "<strong>Aqui</strong>: EER=3.60+-0.15 com os parâmetros do artigo e EER=3.32+-0.17 com os encontrados aqui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] https://arxiv.org/pdf/1604.00974 <br>\n",
    "[2] https://arxiv.org/pdf/1607.04573 <br>\n",
    "[3] https://arxiv.org/pdf/1705.05787"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
